{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecdda31",
   "metadata": {},
   "source": [
    "## Tech Challenge - Reconhecimento facial\n",
    "**Aluno:** Vinícius Oliveira Litran Andrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a25d2",
   "metadata": {},
   "source": [
    "## 📦 Importações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a81e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vinid\\Downloads\\POS\\venv11\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2  # OpenCV: manipulação de vídeos e imagens\n",
    "import numpy as np  # Numpy: operações matemáticas e manipulação de arrays\n",
    "import math  # Math: funções matemáticas como distância euclidiana\n",
    "from deepface import DeepFace  # Biblioteca de reconhecimento e análise facial\n",
    "from collections import Counter, deque  # Contador e fila circular (deque) para histórico\n",
    "import mediapipe as mp  # MediaPipe: detecção de poses corporais\n",
    "from tqdm import tqdm  # Barra de progresso elegante\n",
    "import torch  # PyTorch: para checar disponibilidade de GPU\n",
    "from transformers import pipeline  # HuggingFace Transformers: sumarização textual\n",
    "from PIL import ImageFont, ImageDraw, Image  # Pillow: texto com suporte a acentuação\n",
    "import os  # Módulo para operações no sistema de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b41ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para fonte com suporte a acentos\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\arial.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf6f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário de tradução de emoções\n",
    "EMOTION_TRANSLATION = {\n",
    "    \"happy\": \"Feliz\", \"sad\": \"Triste\", \"angry\": \"Bravo\",\n",
    "    \"surprise\": \"Surpreso\", \"fear\": \"Assustado\",\n",
    "    \"disgust\": \"Com nojo\", \"neutral\": \"Neutro\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5c8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_text_accented(frame, text_data, position=None, font_path=\"C:\\Windows\\Fonts\\arial.ttf\", font_size=24, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Adiciona texto com acentos em um frame OpenCV utilizando Pillow.\n",
    "\n",
    "    Parâmetros:\n",
    "        frame: Imagem (frame) no formato BGR.\n",
    "        text_data: String (texto único) ou lista de tuplas (texto, posição).\n",
    "        position: Tupla (x, y) da posição do texto (se text_data for string).\n",
    "        font_path: Caminho para a fonte .ttf.\n",
    "        font_size: Tamanho da fonte.\n",
    "        color: Cor em formato BGR.\n",
    "    \n",
    "    Retorna:\n",
    "        Frame com texto renderizado corretamente.\n",
    "    \"\"\"\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    if isinstance(text_data, list):\n",
    "        for text, pos in text_data:\n",
    "            draw.text(pos, text, font=font, fill=color[::-1])\n",
    "    else:\n",
    "        if position is None:\n",
    "            position = (10, 10)\n",
    "        draw.text(position, text_data, font=font, fill=color[::-1])\n",
    "\n",
    "    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84ebba",
   "metadata": {},
   "source": [
    "## 🧠 Classe Principal: VideoAnalyzer\n",
    "Responsável por processar o vídeo, detectar emoções, atividades e anomalias, além de salvar os resultados e gerar um resumo textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e6dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAnalyzer:\n",
    "    def __init__(self,\n",
    "                 video_path=\"Unlocking Facial Recognition_ Diverse Activities Analysis.mp4\",\n",
    "                 output_video_path=\"output_video.mp4\",\n",
    "                 summary_path=\"video_summary.txt\",\n",
    "                 emotion_backend=\"retinaface\",\n",
    "                 anomaly_threshold_ratio=0.15,\n",
    "                 frame_skip=1,\n",
    "                 resize_scale=1,\n",
    "                 emotion_history_length=20,\n",
    "                 anomaly_movement_threshold=100,\n",
    "                 missing_face_tolerance=3,\n",
    "                 stability_threshold=4\n",
    "                ):\n",
    "        # Parâmetros configuráveis\n",
    "        self.VIDEO_PATH = video_path  # Caminho do vídeo de entrada\n",
    "        self.OUTPUT_VIDEO_PATH = output_video_path  # Caminho do vídeo anotado\n",
    "        self.SUMMARY_PATH = summary_path  # Caminho do arquivo de resumo (.txt)\n",
    "        self.EMOTION_BACKEND = emotion_backend  # Backend do DeepFace\n",
    "        self.ANOMALY_THRESHOLD_RATIO = anomaly_threshold_ratio  # Área mínima para considerar anomalia facial\n",
    "        self.FRAME_SKIP = frame_skip  # Pula frames para melhorar desempenho\n",
    "        self.RESIZE_SCALE = resize_scale  # Redimensionamento do frame (ajuda no DeepFace)\n",
    "        self.EMOTION_HISTORY_LENGTH = emotion_history_length  # Tamanho do buffer de emoções para suavização\n",
    "        self.ANOMALY_MOVEMENT_THRESHOLD = anomaly_movement_threshold  # Threshold para detectar movimento anômalo\n",
    "        self.MISSING_FACE_TOLERANCE = missing_face_tolerance  # Número de frames sem face antes de considerar \"perdido\"\n",
    "        self.STABILITY_THRESHOLD = stability_threshold  # Número de frames iguais para considerar emoção estável\n",
    "\n",
    "        # Inicialização dos módulos\n",
    "        self.mp_pose = mp.solutions.pose  # Módulo de pose corporal do MediaPipe\n",
    "        self.mp_drawing = mp.solutions.drawing_utils  # Utilitário para desenhar conexões entre landmarks\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Carrega modelo pré-treinado para sumarização de texto\n",
    "\n",
    "        # Checagem de hardware\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"✅ GPU disponível...\")\n",
    "        else:\n",
    "            print(\"⚠️ GPU não disponível...\")\n",
    "\n",
    "        # Estado interno\n",
    "        self.emotion_history = deque(maxlen=self.EMOTION_HISTORY_LENGTH)  # Histórico recente de emoções\n",
    "        self.missing_face_counter = 0  # Contador de frames sem face detectada\n",
    "        self.pose = self.mp_pose.Pose()  # Inicializa detecção de pose corporal\n",
    "\n",
    "\n",
    "\n",
    "    def extract_landmarks(self, results, frame_shape):\n",
    "        \"\"\"\n",
    "        Extrai e retorna landmarks relevantes da pose detectada no frame.\n",
    "        \"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "        landmarks = {}\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if lm.visibility > 0.6:\n",
    "                name = self.mp_pose.PoseLandmark(idx).name\n",
    "                landmarks[name] = (int(lm.x * frame_shape[1]), int(lm.y * frame_shape[0]))\n",
    "        return landmarks\n",
    "\n",
    "    def clamp_coords(self, x, y, w, h, frame_shape):\n",
    "        \"\"\"\n",
    "        Garante que coordenadas não extrapolem os limites da imagem.\n",
    "        \"\"\"\n",
    "        max_x = frame_shape[1] - 1\n",
    "        max_y = frame_shape[0] - 1\n",
    "        x = int(np.clip(x, 0, max_x))\n",
    "        y = int(np.clip(y, 0, max_y))\n",
    "        w = int(np.clip(w, 1, max_x - x))\n",
    "        h = int(np.clip(h, 1, max_y - y))\n",
    "        return x, y, w, h\n",
    "\n",
    "    def analyze_emotions(self, small_frame, full_frame, emotion_summary, anomalies_detected, frame_area):\n",
    "        \"\"\"\n",
    "        Analisa emoções detectadas via DeepFace, atualiza contadores e desenha no frame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(\n",
    "                small_frame,\n",
    "                actions=['emotion'],\n",
    "                enforce_detection=False,\n",
    "                detector_backend=self.EMOTION_BACKEND\n",
    "            )\n",
    "\n",
    "            if isinstance(analysis, dict):\n",
    "                analysis = [analysis]\n",
    "\n",
    "            emotion_texts = []\n",
    "\n",
    "            for result in analysis:\n",
    "                region = result.get(\"region\")\n",
    "                emotion = result.get(\"dominant_emotion\")\n",
    "                if not region or not emotion:\n",
    "                    continue\n",
    "\n",
    "                x = int(region['x'] / self.RESIZE_SCALE)\n",
    "                y = int(region['y'] / self.RESIZE_SCALE)\n",
    "                w = int(region['w'] / self.RESIZE_SCALE)\n",
    "                h = int(region['h'] / self.RESIZE_SCALE)\n",
    "\n",
    "                x, y, w, h = self.clamp_coords(x, y, w, h, full_frame.shape)\n",
    "                area_face = w * h\n",
    "\n",
    "                if area_face > self.ANOMALY_THRESHOLD_RATIO * frame_area:\n",
    "                    anomalies_detected += 1\n",
    "\n",
    "                emotion_pt = EMOTION_TRANSLATION.get(emotion.lower(), emotion)\n",
    "                emotion_summary[emotion_pt] += 1\n",
    "\n",
    "                cv2.rectangle(full_frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "                emotion_texts.append((emotion_pt, (x, max(y - 30, 0))))\n",
    "\n",
    "            if emotion_texts:\n",
    "                full_frame = put_text_accented(full_frame, emotion_texts, font_path=FONT_PATH, font_size=24, color=(0,255,0))\n",
    "\n",
    "            if not analysis or all(r.get(\"region\") is None for r in analysis):\n",
    "                self.missing_face_counter += 1\n",
    "            else:\n",
    "                self.missing_face_counter = 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Falha na análise emocional: {str(e)}\")\n",
    "\n",
    "        return anomalies_detected, full_frame\n",
    "\n",
    "\n",
    "\n",
    "    def detect_activity_from_landmarks(self, landmarks, image_shape):\n",
    "        \"\"\"\n",
    "        Detecta atividades básicas a partir dos landmarks do corpo.\n",
    "        \"\"\"\n",
    "        activities = []\n",
    "        h, w = image_shape\n",
    "\n",
    "        def get_landmark(name):\n",
    "            return landmarks.get(name, None)\n",
    "\n",
    "        def distance(p1, p2):\n",
    "            if p1 and p2:\n",
    "                return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
    "            return float('inf')\n",
    "\n",
    "        def is_near(p1, p2, threshold=0.05):\n",
    "            return distance(p1, p2) < threshold * w\n",
    "\n",
    "        def is_above(p1, p2, offset=0.1):\n",
    "            return p1 and p2 and p1[1] < p2[1] - offset * h\n",
    "\n",
    "        def is_aligned_vertically(p1, p2, tolerance=0.05):\n",
    "            return p1 and p2 and abs(p1[0] - p2[0]) < tolerance * w\n",
    "\n",
    "        left_shoulder = get_landmark('LEFT_SHOULDER')\n",
    "        right_shoulder = get_landmark('RIGHT_SHOULDER')\n",
    "        left_hip = get_landmark('LEFT_HIP')\n",
    "        right_hip = get_landmark('RIGHT_HIP')\n",
    "        left_wrist = get_landmark('LEFT_WRIST')\n",
    "        right_wrist = get_landmark('RIGHT_WRIST')\n",
    "        left_hand = get_landmark('LEFT_INDEX')\n",
    "        right_hand = get_landmark('RIGHT_INDEX')\n",
    "        nose = get_landmark('NOSE')\n",
    "        mouth_left = get_landmark('MOUTH_LEFT')\n",
    "        mouth_right = get_landmark('MOUTH_RIGHT')\n",
    "        left_eye = get_landmark('LEFT_EYE')\n",
    "        right_eye = get_landmark('RIGHT_EYE')\n",
    "\n",
    "        # Postura: sentado / em pé\n",
    "        if left_shoulder and right_shoulder and left_hip and right_hip:\n",
    "            shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "            hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "            torso_length = abs(hip_y - shoulder_y)\n",
    "            if hip_y > shoulder_y + 0.12 * h and torso_length > 0.25 * h:\n",
    "                activities.append(\"sentado\")\n",
    "            elif torso_length > 0.35 * h and abs(hip_y - shoulder_y) < 0.15 * h:\n",
    "                activities.append(\"em pé\")\n",
    "\n",
    "        # Gestos manuais\n",
    "        if is_above(left_wrist, left_shoulder) and left_wrist[1] < 0.5 * h:\n",
    "            activities.append(\"acenando com a mão esquerda\")\n",
    "        if is_above(right_wrist, right_shoulder) and right_wrist[1] < 0.5 * h:\n",
    "            activities.append(\"acenando com a mão direita\")\n",
    "\n",
    "        if left_wrist and right_shoulder and abs(left_wrist[1] - right_shoulder[1]) < 0.07 * h and left_wrist[1] > 0.4 * h:\n",
    "            activities.append(\"gesticulando com a mão esquerda\")\n",
    "        if right_wrist and left_shoulder and abs(right_wrist[1] - left_shoulder[1]) < 0.07 * h and right_wrist[1] > 0.4 * h:\n",
    "            activities.append(\"gesticulando com a mão direita\")\n",
    "\n",
    "        if left_wrist and right_wrist and abs(left_wrist[0] - right_wrist[0]) > 0.35 * w and abs(left_wrist[1] - right_wrist[1]) < 0.15 * h:\n",
    "            activities.append(\"dançando\")\n",
    "\n",
    "        if left_hand and right_hand and is_near(left_hand, right_hand):\n",
    "            if abs(left_hand[1] - right_hand[1]) < 0.05 * h:\n",
    "                activities.append(\"apertando as mãos\")\n",
    "\n",
    "        # Bocejo: boca aberta verticalmente e boca estreita horizontalmente\n",
    "        if nose and mouth_left and mouth_right:\n",
    "            mouth_center_y = (mouth_left[1] + mouth_right[1]) / 2\n",
    "            mouth_width = distance(mouth_left, mouth_right)\n",
    "            mouth_height = abs(nose[1] - mouth_center_y)\n",
    "            aspect_ratio = mouth_height / (mouth_width + 1e-6)\n",
    "            if mouth_height > 0.12 * h and aspect_ratio > 0.65:\n",
    "                activities.append(\"bocejando\")\n",
    "\n",
    "        # Riso: boca larga e olhos levemente mais baixos que nariz\n",
    "        if nose and mouth_left and mouth_right and left_eye and right_eye:\n",
    "            mouth_width = distance(mouth_left, mouth_right)\n",
    "            eye_avg_y = (left_eye[1] + right_eye[1]) / 2\n",
    "            mouth_center_y = (mouth_left[1] + mouth_right[1]) / 2\n",
    "            eye_to_nose = eye_avg_y - nose[1]\n",
    "            if mouth_width > 0.18 * w and 0.01 * h < eye_to_nose < 0.06 * h:\n",
    "                activities.append(\"rindo\")\n",
    "\n",
    "        # Escrevendo: mãos alinhadas verticalmente e próximas abaixo do nariz\n",
    "        if nose and left_wrist and right_wrist:\n",
    "            if (abs(left_wrist[1] - right_wrist[1]) < 0.1 * h and\n",
    "                is_aligned_vertically(left_wrist, right_wrist) and\n",
    "                nose[1] + 0.05 * h < min(left_wrist[1], right_wrist[1])):\n",
    "                activities.append(\"escrevendo\")\n",
    "\n",
    "        return activities\n",
    "\n",
    "    def calculate_landmark_movement(self, current_landmarks, previous_landmarks):\n",
    "        \"\"\"\n",
    "        Calcula movimento médio ao quadrado entre dois conjuntos de landmarks.\n",
    "        \"\"\"\n",
    "        if not current_landmarks or not previous_landmarks:\n",
    "            return 0\n",
    "        total_movement = 0\n",
    "        count = 0\n",
    "        for key in current_landmarks:\n",
    "            if key in previous_landmarks:\n",
    "                x1, y1 = current_landmarks[key]\n",
    "                x2, y2 = previous_landmarks[key]\n",
    "                dx = x1 - x2\n",
    "                dy = y1 - y2\n",
    "                total_movement += dx * dx + dy * dy\n",
    "                count += 1\n",
    "        return total_movement / count if count > 0 else 0\n",
    "\n",
    "    def generate_summary(self, summary_path, video_name, processed_frames, anomalies_detected, emotion_summary, activity_summary):\n",
    "        \"\"\"\n",
    "        Gera arquivo de resumo com estatísticas e resumo textual.\n",
    "        \"\"\"\n",
    "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"=== Resumo da Análise de Vídeo ===\\n\")\n",
    "            f.write(f\"Vídeo analisado: {video_name}\\n\")\n",
    "            f.write(f\"Total de frames processados: {processed_frames}\\n\")\n",
    "            f.write(f\"Número de anomalias detectadas: {anomalies_detected}\\n\\n\")\n",
    "\n",
    "            f.write(\"Distribuição de emoções:\\n\")\n",
    "            for emotion, count in emotion_summary.most_common():\n",
    "                f.write(f\"  {emotion}: {count}\\n\")\n",
    "\n",
    "            f.write(\"\\nAtividades detectadas:\\n\")\n",
    "            for activity, count in activity_summary.most_common():\n",
    "                f.write(f\"  {activity}: {count}\\n\")\n",
    "\n",
    "            # Gera prompt para resumo\n",
    "            emotion_list = \", \".join(f\"{k} ({v})\" for k, v in emotion_summary.most_common(5))\n",
    "            activity_list = \", \".join(f\"{k} ({v})\" for k, v in activity_summary.most_common(5))\n",
    "            prompt = f\"Durante o vídeo foram detectadas as seguintes emoções: {emotion_list}. As atividades mais comuns foram: {activity_list}.\"\n",
    "\n",
    "            try:\n",
    "                resumo = self.summarizer(prompt, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao gerar resumo automático: {e}\")\n",
    "                resumo = \"Resumo automático não disponível.\"\n",
    "\n",
    "            f.write(\"\\nResumo em linguagem natural:\\n\")\n",
    "            f.write(resumo + \"\\n\")\n",
    "\n",
    "    def process_video(self):\n",
    "        \"\"\"\n",
    "        Loop principal de processamento do vídeo:\n",
    "        - Captura frame a frame\n",
    "        - Aplica análise de emoções, atividades e movimentos\n",
    "        - Gera vídeo anotado e resumo final\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(self.VIDEO_PATH)\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(f\"Erro ao abrir o vídeo: {self.VIDEO_PATH}\")\n",
    "\n",
    "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(self.OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            activity_summary = Counter()\n",
    "            emotion_summary = Counter()\n",
    "            anomalies_detected = 0\n",
    "            last_landmarks = None\n",
    "            frame_area = width * height\n",
    "\n",
    "            for i in tqdm(range(frame_count), desc=\"🔍 Processando vídeo\"):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = self.pose.process(rgb_frame)\n",
    "\n",
    "                current_landmarks = self.extract_landmarks(results, frame.shape)\n",
    "\n",
    "                overlay_text = \"\"\n",
    "                is_anomalous = False\n",
    "                activities = []\n",
    "\n",
    "                if i % self.FRAME_SKIP == 0:\n",
    "                    small_frame = cv2.resize(frame, (0, 0), fx=self.RESIZE_SCALE, fy=self.RESIZE_SCALE)\n",
    "                    anomalies_detected, frame = self.analyze_emotions(\n",
    "                        small_frame, frame, emotion_summary, anomalies_detected, frame_area)\n",
    "\n",
    "                if current_landmarks:\n",
    "                    activities = self.detect_activity_from_landmarks(current_landmarks, frame.shape[:2])\n",
    "                    for act in activities:\n",
    "                        activity_summary[act] += 1\n",
    "\n",
    "                movement_score = 0\n",
    "                if current_landmarks and last_landmarks:\n",
    "                    movement_score = self.calculate_landmark_movement(current_landmarks, last_landmarks)\n",
    "\n",
    "                if movement_score > self.ANOMALY_MOVEMENT_THRESHOLD and not activities:\n",
    "                    is_anomalous = True\n",
    "                    anomalies_detected += 1\n",
    "                    overlay_text += \"⚠️ Movimento anômalo detectado\\n\"\n",
    "                elif activities:\n",
    "                    overlay_text += \"Atividades: \" + \", \".join(activities) + \"\\n\"\n",
    "\n",
    "                if (movement_score > self.ANOMALY_MOVEMENT_THRESHOLD or activities) and results.pose_landmarks:\n",
    "                    self.mp_drawing.draw_landmarks(frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                last_landmarks = current_landmarks\n",
    "\n",
    "                if overlay_text:\n",
    "                    y0 = 30\n",
    "                    for idx, line in enumerate(overlay_text.strip().split('\\n')):\n",
    "                        y = y0 + idx * 30\n",
    "                        frame = put_text_accented(frame, line, (30, y), font_path=FONT_PATH, font_size=22, color=(0, 0, 255))\n",
    "\n",
    "                out.write(frame)\n",
    "\n",
    "            cap.release()\n",
    "            out.release()\n",
    "\n",
    "            self.generate_summary(self.SUMMARY_PATH, self.VIDEO_PATH, frame_count, anomalies_detected, emotion_summary, activity_summary)\n",
    "\n",
    "            print(f\"✅ Análise concluída. {anomalies_detected} anomalias detectadas.\")\n",
    "            print(f\"Resumo salvo em '{self.SUMMARY_PATH}', vídeo anotado em '{self.OUTPUT_VIDEO_PATH}'.\")\n",
    "\n",
    "        finally:\n",
    "            self.pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a36c2",
   "metadata": {},
   "source": [
    "## 🚀 Execução principal\n",
    "Instancia a classe e inicia o processamento completo do vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f069f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU disponível...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Processando vídeo: 100%|██████████| 3326/3326 [56:51<00:00,  1.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Análise concluída. 1430 anomalias detectadas.\n",
      "Resumo salvo em 'video_summary.txt', vídeo anotado em 'output_video.mp4'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    analyzer = VideoAnalyzer()\n",
    "    analyzer.process_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
