{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecdda31",
   "metadata": {},
   "source": [
    "## Tech Challenge - Reconhecimento facial\n",
    "**Aluno:** Vin√≠cius Oliveira Litran Andrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a25d2",
   "metadata": {},
   "source": [
    "## üì¶ Importa√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a81e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vinid\\Downloads\\POS\\venv11\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2  # OpenCV: manipula√ß√£o de v√≠deos e imagens\n",
    "import numpy as np  # Numpy: opera√ß√µes matem√°ticas e manipula√ß√£o de arrays\n",
    "import math  # Math: fun√ß√µes matem√°ticas como dist√¢ncia euclidiana\n",
    "from deepface import DeepFace  # Biblioteca de reconhecimento e an√°lise facial\n",
    "from collections import Counter, deque  # Contador e fila circular (deque) para hist√≥rico\n",
    "import mediapipe as mp  # MediaPipe: detec√ß√£o de poses corporais\n",
    "from tqdm import tqdm  # Barra de progresso elegante\n",
    "import torch  # PyTorch: para checar disponibilidade de GPU\n",
    "from transformers import pipeline  # HuggingFace Transformers: sumariza√ß√£o textual\n",
    "from PIL import ImageFont, ImageDraw, Image  # Pillow: texto com suporte a acentua√ß√£o\n",
    "import os  # M√≥dulo para opera√ß√µes no sistema de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b41ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para fonte com suporte a acentos\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\arial.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf6f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicion√°rio de tradu√ß√£o de emo√ß√µes\n",
    "EMOTION_TRANSLATION = {\n",
    "    \"happy\": \"Feliz\", \"sad\": \"Triste\", \"angry\": \"Bravo\",\n",
    "    \"surprise\": \"Surpreso\", \"fear\": \"Assustado\",\n",
    "    \"disgust\": \"Com nojo\", \"neutral\": \"Neutro\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5c8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_text_accented(frame, text_data, position=None, font_path=\"C:\\Windows\\Fonts\\arial.ttf\", font_size=24, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Adiciona texto com acentos em um frame OpenCV utilizando Pillow.\n",
    "\n",
    "    Par√¢metros:\n",
    "        frame: Imagem (frame) no formato BGR.\n",
    "        text_data: String (texto √∫nico) ou lista de tuplas (texto, posi√ß√£o).\n",
    "        position: Tupla (x, y) da posi√ß√£o do texto (se text_data for string).\n",
    "        font_path: Caminho para a fonte .ttf.\n",
    "        font_size: Tamanho da fonte.\n",
    "        color: Cor em formato BGR.\n",
    "    \n",
    "    Retorna:\n",
    "        Frame com texto renderizado corretamente.\n",
    "    \"\"\"\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    if isinstance(text_data, list):\n",
    "        for text, pos in text_data:\n",
    "            draw.text(pos, text, font=font, fill=color[::-1])\n",
    "    else:\n",
    "        if position is None:\n",
    "            position = (10, 10)\n",
    "        draw.text(position, text_data, font=font, fill=color[::-1])\n",
    "\n",
    "    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84ebba",
   "metadata": {},
   "source": [
    "## üß† Classe Principal: VideoAnalyzer\n",
    "Respons√°vel por processar o v√≠deo, detectar emo√ß√µes, atividades e anomalias, al√©m de salvar os resultados e gerar um resumo textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e6dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAnalyzer:\n",
    "    def __init__(self,\n",
    "                 video_path=\"Unlocking Facial Recognition_ Diverse Activities Analysis.mp4\",\n",
    "                 output_video_path=\"output_video.mp4\",\n",
    "                 summary_path=\"video_summary.txt\",\n",
    "                 emotion_backend=\"retinaface\",\n",
    "                 anomaly_threshold_ratio=0.15,\n",
    "                 frame_skip=1,\n",
    "                 resize_scale=1,\n",
    "                 emotion_history_length=20,\n",
    "                 anomaly_movement_threshold=100,\n",
    "                 missing_face_tolerance=3,\n",
    "                 stability_threshold=4\n",
    "                ):\n",
    "        # Par√¢metros configur√°veis\n",
    "        self.VIDEO_PATH = video_path  # Caminho do v√≠deo de entrada\n",
    "        self.OUTPUT_VIDEO_PATH = output_video_path  # Caminho do v√≠deo anotado\n",
    "        self.SUMMARY_PATH = summary_path  # Caminho do arquivo de resumo (.txt)\n",
    "        self.EMOTION_BACKEND = emotion_backend  # Backend do DeepFace\n",
    "        self.ANOMALY_THRESHOLD_RATIO = anomaly_threshold_ratio  # √Årea m√≠nima para considerar anomalia facial\n",
    "        self.FRAME_SKIP = frame_skip  # Pula frames para melhorar desempenho\n",
    "        self.RESIZE_SCALE = resize_scale  # Redimensionamento do frame (ajuda no DeepFace)\n",
    "        self.EMOTION_HISTORY_LENGTH = emotion_history_length  # Tamanho do buffer de emo√ß√µes para suaviza√ß√£o\n",
    "        self.ANOMALY_MOVEMENT_THRESHOLD = anomaly_movement_threshold  # Threshold para detectar movimento an√¥malo\n",
    "        self.MISSING_FACE_TOLERANCE = missing_face_tolerance  # N√∫mero de frames sem face antes de considerar \"perdido\"\n",
    "        self.STABILITY_THRESHOLD = stability_threshold  # N√∫mero de frames iguais para considerar emo√ß√£o est√°vel\n",
    "\n",
    "        # Inicializa√ß√£o dos m√≥dulos\n",
    "        self.mp_pose = mp.solutions.pose  # M√≥dulo de pose corporal do MediaPipe\n",
    "        self.mp_drawing = mp.solutions.drawing_utils  # Utilit√°rio para desenhar conex√µes entre landmarks\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Carrega modelo pr√©-treinado para sumariza√ß√£o de texto\n",
    "\n",
    "        # Checagem de hardware\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"‚úÖ GPU dispon√≠vel...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è GPU n√£o dispon√≠vel...\")\n",
    "\n",
    "        # Estado interno\n",
    "        self.emotion_history = deque(maxlen=self.EMOTION_HISTORY_LENGTH)  # Hist√≥rico recente de emo√ß√µes\n",
    "        self.missing_face_counter = 0  # Contador de frames sem face detectada\n",
    "        self.pose = self.mp_pose.Pose()  # Inicializa detec√ß√£o de pose corporal\n",
    "\n",
    "\n",
    "\n",
    "    def extract_landmarks(self, results, frame_shape):\n",
    "        \"\"\"\n",
    "        Extrai e retorna landmarks relevantes da pose detectada no frame.\n",
    "        \"\"\"\n",
    "        if not results.pose_landmarks:\n",
    "            return None\n",
    "        landmarks = {}\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if lm.visibility > 0.6:\n",
    "                name = self.mp_pose.PoseLandmark(idx).name\n",
    "                landmarks[name] = (int(lm.x * frame_shape[1]), int(lm.y * frame_shape[0]))\n",
    "        return landmarks\n",
    "\n",
    "    def clamp_coords(self, x, y, w, h, frame_shape):\n",
    "        \"\"\"\n",
    "        Garante que coordenadas n√£o extrapolem os limites da imagem.\n",
    "        \"\"\"\n",
    "        max_x = frame_shape[1] - 1\n",
    "        max_y = frame_shape[0] - 1\n",
    "        x = int(np.clip(x, 0, max_x))\n",
    "        y = int(np.clip(y, 0, max_y))\n",
    "        w = int(np.clip(w, 1, max_x - x))\n",
    "        h = int(np.clip(h, 1, max_y - y))\n",
    "        return x, y, w, h\n",
    "\n",
    "    def analyze_emotions(self, small_frame, full_frame, emotion_summary, anomalies_detected, frame_area):\n",
    "        \"\"\"\n",
    "        Analisa emo√ß√µes detectadas via DeepFace, atualiza contadores e desenha no frame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(\n",
    "                small_frame,\n",
    "                actions=['emotion'],\n",
    "                enforce_detection=False,\n",
    "                detector_backend=self.EMOTION_BACKEND\n",
    "            )\n",
    "\n",
    "            if isinstance(analysis, dict):\n",
    "                analysis = [analysis]\n",
    "\n",
    "            emotion_texts = []\n",
    "\n",
    "            for result in analysis:\n",
    "                region = result.get(\"region\")\n",
    "                emotion = result.get(\"dominant_emotion\")\n",
    "                if not region or not emotion:\n",
    "                    continue\n",
    "\n",
    "                x = int(region['x'] / self.RESIZE_SCALE)\n",
    "                y = int(region['y'] / self.RESIZE_SCALE)\n",
    "                w = int(region['w'] / self.RESIZE_SCALE)\n",
    "                h = int(region['h'] / self.RESIZE_SCALE)\n",
    "\n",
    "                x, y, w, h = self.clamp_coords(x, y, w, h, full_frame.shape)\n",
    "                area_face = w * h\n",
    "\n",
    "                if area_face > self.ANOMALY_THRESHOLD_RATIO * frame_area:\n",
    "                    anomalies_detected += 1\n",
    "\n",
    "                emotion_pt = EMOTION_TRANSLATION.get(emotion.lower(), emotion)\n",
    "                emotion_summary[emotion_pt] += 1\n",
    "\n",
    "                cv2.rectangle(full_frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "                emotion_texts.append((emotion_pt, (x, max(y - 30, 0))))\n",
    "\n",
    "            if emotion_texts:\n",
    "                full_frame = put_text_accented(full_frame, emotion_texts, font_path=FONT_PATH, font_size=24, color=(0,255,0))\n",
    "\n",
    "            if not analysis or all(r.get(\"region\") is None for r in analysis):\n",
    "                self.missing_face_counter += 1\n",
    "            else:\n",
    "                self.missing_face_counter = 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Falha na an√°lise emocional: {str(e)}\")\n",
    "\n",
    "        return anomalies_detected, full_frame\n",
    "\n",
    "\n",
    "\n",
    "    def detect_activity_from_landmarks(self, landmarks, image_shape):\n",
    "        \"\"\"\n",
    "        Detecta atividades b√°sicas a partir dos landmarks do corpo.\n",
    "        \"\"\"\n",
    "        activities = []\n",
    "        h, w = image_shape\n",
    "\n",
    "        def get_landmark(name):\n",
    "            return landmarks.get(name, None)\n",
    "\n",
    "        def distance(p1, p2):\n",
    "            if p1 and p2:\n",
    "                return math.hypot(p1[0] - p2[0], p1[1] - p2[1])\n",
    "            return float('inf')\n",
    "\n",
    "        def is_near(p1, p2, threshold=0.05):\n",
    "            return distance(p1, p2) < threshold * w\n",
    "\n",
    "        def is_above(p1, p2, offset=0.1):\n",
    "            return p1 and p2 and p1[1] < p2[1] - offset * h\n",
    "\n",
    "        def is_aligned_vertically(p1, p2, tolerance=0.05):\n",
    "            return p1 and p2 and abs(p1[0] - p2[0]) < tolerance * w\n",
    "\n",
    "        left_shoulder = get_landmark('LEFT_SHOULDER')\n",
    "        right_shoulder = get_landmark('RIGHT_SHOULDER')\n",
    "        left_hip = get_landmark('LEFT_HIP')\n",
    "        right_hip = get_landmark('RIGHT_HIP')\n",
    "        left_wrist = get_landmark('LEFT_WRIST')\n",
    "        right_wrist = get_landmark('RIGHT_WRIST')\n",
    "        left_hand = get_landmark('LEFT_INDEX')\n",
    "        right_hand = get_landmark('RIGHT_INDEX')\n",
    "        nose = get_landmark('NOSE')\n",
    "        mouth_left = get_landmark('MOUTH_LEFT')\n",
    "        mouth_right = get_landmark('MOUTH_RIGHT')\n",
    "        left_eye = get_landmark('LEFT_EYE')\n",
    "        right_eye = get_landmark('RIGHT_EYE')\n",
    "\n",
    "        # Postura: sentado / em p√©\n",
    "        if left_shoulder and right_shoulder and left_hip and right_hip:\n",
    "            shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "            hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "            torso_length = abs(hip_y - shoulder_y)\n",
    "            if hip_y > shoulder_y + 0.12 * h and torso_length > 0.25 * h:\n",
    "                activities.append(\"sentado\")\n",
    "            elif torso_length > 0.35 * h and abs(hip_y - shoulder_y) < 0.15 * h:\n",
    "                activities.append(\"em p√©\")\n",
    "\n",
    "        # Gestos manuais\n",
    "        if is_above(left_wrist, left_shoulder) and left_wrist[1] < 0.5 * h:\n",
    "            activities.append(\"acenando com a m√£o esquerda\")\n",
    "        if is_above(right_wrist, right_shoulder) and right_wrist[1] < 0.5 * h:\n",
    "            activities.append(\"acenando com a m√£o direita\")\n",
    "\n",
    "        if left_wrist and right_shoulder and abs(left_wrist[1] - right_shoulder[1]) < 0.07 * h and left_wrist[1] > 0.4 * h:\n",
    "            activities.append(\"gesticulando com a m√£o esquerda\")\n",
    "        if right_wrist and left_shoulder and abs(right_wrist[1] - left_shoulder[1]) < 0.07 * h and right_wrist[1] > 0.4 * h:\n",
    "            activities.append(\"gesticulando com a m√£o direita\")\n",
    "\n",
    "        if left_wrist and right_wrist and abs(left_wrist[0] - right_wrist[0]) > 0.35 * w and abs(left_wrist[1] - right_wrist[1]) < 0.15 * h:\n",
    "            activities.append(\"dan√ßando\")\n",
    "\n",
    "        if left_hand and right_hand and is_near(left_hand, right_hand):\n",
    "            if abs(left_hand[1] - right_hand[1]) < 0.05 * h:\n",
    "                activities.append(\"apertando as m√£os\")\n",
    "\n",
    "        # Bocejo: boca aberta verticalmente e boca estreita horizontalmente\n",
    "        if nose and mouth_left and mouth_right:\n",
    "            mouth_center_y = (mouth_left[1] + mouth_right[1]) / 2\n",
    "            mouth_width = distance(mouth_left, mouth_right)\n",
    "            mouth_height = abs(nose[1] - mouth_center_y)\n",
    "            aspect_ratio = mouth_height / (mouth_width + 1e-6)\n",
    "            if mouth_height > 0.12 * h and aspect_ratio > 0.65:\n",
    "                activities.append(\"bocejando\")\n",
    "\n",
    "        # Riso: boca larga e olhos levemente mais baixos que nariz\n",
    "        if nose and mouth_left and mouth_right and left_eye and right_eye:\n",
    "            mouth_width = distance(mouth_left, mouth_right)\n",
    "            eye_avg_y = (left_eye[1] + right_eye[1]) / 2\n",
    "            mouth_center_y = (mouth_left[1] + mouth_right[1]) / 2\n",
    "            eye_to_nose = eye_avg_y - nose[1]\n",
    "            if mouth_width > 0.18 * w and 0.01 * h < eye_to_nose < 0.06 * h:\n",
    "                activities.append(\"rindo\")\n",
    "\n",
    "        # Escrevendo: m√£os alinhadas verticalmente e pr√≥ximas abaixo do nariz\n",
    "        if nose and left_wrist and right_wrist:\n",
    "            if (abs(left_wrist[1] - right_wrist[1]) < 0.1 * h and\n",
    "                is_aligned_vertically(left_wrist, right_wrist) and\n",
    "                nose[1] + 0.05 * h < min(left_wrist[1], right_wrist[1])):\n",
    "                activities.append(\"escrevendo\")\n",
    "\n",
    "        return activities\n",
    "\n",
    "    def calculate_landmark_movement(self, current_landmarks, previous_landmarks):\n",
    "        \"\"\"\n",
    "        Calcula movimento m√©dio ao quadrado entre dois conjuntos de landmarks.\n",
    "        \"\"\"\n",
    "        if not current_landmarks or not previous_landmarks:\n",
    "            return 0\n",
    "        total_movement = 0\n",
    "        count = 0\n",
    "        for key in current_landmarks:\n",
    "            if key in previous_landmarks:\n",
    "                x1, y1 = current_landmarks[key]\n",
    "                x2, y2 = previous_landmarks[key]\n",
    "                dx = x1 - x2\n",
    "                dy = y1 - y2\n",
    "                total_movement += dx * dx + dy * dy\n",
    "                count += 1\n",
    "        return total_movement / count if count > 0 else 0\n",
    "\n",
    "    def generate_summary(self, summary_path, video_name, processed_frames, anomalies_detected, emotion_summary, activity_summary):\n",
    "        \"\"\"\n",
    "        Gera arquivo de resumo com estat√≠sticas e resumo textual.\n",
    "        \"\"\"\n",
    "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"=== Resumo da An√°lise de V√≠deo ===\\n\")\n",
    "            f.write(f\"V√≠deo analisado: {video_name}\\n\")\n",
    "            f.write(f\"Total de frames processados: {processed_frames}\\n\")\n",
    "            f.write(f\"N√∫mero de anomalias detectadas: {anomalies_detected}\\n\\n\")\n",
    "\n",
    "            f.write(\"Distribui√ß√£o de emo√ß√µes:\\n\")\n",
    "            for emotion, count in emotion_summary.most_common():\n",
    "                f.write(f\"  {emotion}: {count}\\n\")\n",
    "\n",
    "            f.write(\"\\nAtividades detectadas:\\n\")\n",
    "            for activity, count in activity_summary.most_common():\n",
    "                f.write(f\"  {activity}: {count}\\n\")\n",
    "\n",
    "            # Gera prompt para resumo\n",
    "            emotion_list = \", \".join(f\"{k} ({v})\" for k, v in emotion_summary.most_common(5))\n",
    "            activity_list = \", \".join(f\"{k} ({v})\" for k, v in activity_summary.most_common(5))\n",
    "            prompt = f\"Durante o v√≠deo foram detectadas as seguintes emo√ß√µes: {emotion_list}. As atividades mais comuns foram: {activity_list}.\"\n",
    "\n",
    "            try:\n",
    "                resumo = self.summarizer(prompt, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao gerar resumo autom√°tico: {e}\")\n",
    "                resumo = \"Resumo autom√°tico n√£o dispon√≠vel.\"\n",
    "\n",
    "            f.write(\"\\nResumo em linguagem natural:\\n\")\n",
    "            f.write(resumo + \"\\n\")\n",
    "\n",
    "    def process_video(self):\n",
    "        \"\"\"\n",
    "        Loop principal de processamento do v√≠deo:\n",
    "        - Captura frame a frame\n",
    "        - Aplica an√°lise de emo√ß√µes, atividades e movimentos\n",
    "        - Gera v√≠deo anotado e resumo final\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(self.VIDEO_PATH)\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(f\"Erro ao abrir o v√≠deo: {self.VIDEO_PATH}\")\n",
    "\n",
    "            fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(self.OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            activity_summary = Counter()\n",
    "            emotion_summary = Counter()\n",
    "            anomalies_detected = 0\n",
    "            last_landmarks = None\n",
    "            frame_area = width * height\n",
    "\n",
    "            for i in tqdm(range(frame_count), desc=\"üîç Processando v√≠deo\"):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = self.pose.process(rgb_frame)\n",
    "\n",
    "                current_landmarks = self.extract_landmarks(results, frame.shape)\n",
    "\n",
    "                overlay_text = \"\"\n",
    "                is_anomalous = False\n",
    "                activities = []\n",
    "\n",
    "                if i % self.FRAME_SKIP == 0:\n",
    "                    small_frame = cv2.resize(frame, (0, 0), fx=self.RESIZE_SCALE, fy=self.RESIZE_SCALE)\n",
    "                    anomalies_detected, frame = self.analyze_emotions(\n",
    "                        small_frame, frame, emotion_summary, anomalies_detected, frame_area)\n",
    "\n",
    "                if current_landmarks:\n",
    "                    activities = self.detect_activity_from_landmarks(current_landmarks, frame.shape[:2])\n",
    "                    for act in activities:\n",
    "                        activity_summary[act] += 1\n",
    "\n",
    "                movement_score = 0\n",
    "                if current_landmarks and last_landmarks:\n",
    "                    movement_score = self.calculate_landmark_movement(current_landmarks, last_landmarks)\n",
    "\n",
    "                if movement_score > self.ANOMALY_MOVEMENT_THRESHOLD and not activities:\n",
    "                    is_anomalous = True\n",
    "                    anomalies_detected += 1\n",
    "                    overlay_text += \"‚ö†Ô∏è Movimento an√¥malo detectado\\n\"\n",
    "                elif activities:\n",
    "                    overlay_text += \"Atividades: \" + \", \".join(activities) + \"\\n\"\n",
    "\n",
    "                if (movement_score > self.ANOMALY_MOVEMENT_THRESHOLD or activities) and results.pose_landmarks:\n",
    "                    self.mp_drawing.draw_landmarks(frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                last_landmarks = current_landmarks\n",
    "\n",
    "                if overlay_text:\n",
    "                    y0 = 30\n",
    "                    for idx, line in enumerate(overlay_text.strip().split('\\n')):\n",
    "                        y = y0 + idx * 30\n",
    "                        frame = put_text_accented(frame, line, (30, y), font_path=FONT_PATH, font_size=22, color=(0, 0, 255))\n",
    "\n",
    "                out.write(frame)\n",
    "\n",
    "            cap.release()\n",
    "            out.release()\n",
    "\n",
    "            self.generate_summary(self.SUMMARY_PATH, self.VIDEO_PATH, frame_count, anomalies_detected, emotion_summary, activity_summary)\n",
    "\n",
    "            print(f\"‚úÖ An√°lise conclu√≠da. {anomalies_detected} anomalias detectadas.\")\n",
    "            print(f\"Resumo salvo em '{self.SUMMARY_PATH}', v√≠deo anotado em '{self.OUTPUT_VIDEO_PATH}'.\")\n",
    "\n",
    "        finally:\n",
    "            self.pose.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a36c2",
   "metadata": {},
   "source": [
    "## üöÄ Execu√ß√£o principal\n",
    "Instancia a classe e inicia o processamento completo do v√≠deo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f069f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU dispon√≠vel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Processando v√≠deo: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3326/3326 [56:51<00:00,  1.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ An√°lise conclu√≠da. 1430 anomalias detectadas.\n",
      "Resumo salvo em 'video_summary.txt', v√≠deo anotado em 'output_video.mp4'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    analyzer = VideoAnalyzer()\n",
    "    analyzer.process_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
